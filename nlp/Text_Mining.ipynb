{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import chunk\n",
    "from nltk.tag.perceptron import PerceptronTagger\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "import statsmodels.api as sm\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import gensim\n",
    "import re\n",
    "import string\n",
    "from toolz import frequencies\n",
    "from multiprocessing import Pool\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from amazon_utils.keywords_crawler import query_page\n",
    "from amazon_utils.crawler_config import CrawlConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def data_load(filepath):\n",
    "    start_time = time.time()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        dataload = pickle.load(f)\n",
    "    end_time = time.time()\n",
    "    print('loaddata use timeï¼š' + str(end_time - start_time) + 's')\n",
    "    return dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a sample English sentence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        rows = f.readlines()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenizer(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a list of sentences\n",
    "    '''\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a list of words\n",
    "    '''\n",
    "    return word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a string without numbers\n",
    "    '''\n",
    "    return re.sub(r'\\d+','', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a string without punctuation\n",
    "    '''\n",
    "    words = word_tokenizer(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return ' '.join(punt_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, lang='english'):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a string without stopwords\n",
    "    '''\n",
    "    words = word_tokenizer(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    return ' '.join(stopwords_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_whitespace(text):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a string without extra whitespaces\n",
    "    '''\n",
    "    return ' '.join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(text, tagger='M'):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a list of tuples, each tuple with a word and each pos\n",
    "    '''\n",
    "    if tagger == 'M':\n",
    "        # M means Maxnet Entropy Algorithm\n",
    "        return nltk.pos_tag(word_tokenizer(text))\n",
    "    if tagger == 'P':\n",
    "        # P means Average Perceptrop Algorithm\n",
    "        PT = PerceptronTagger()\n",
    "        return PT.tag(word_tokenizer(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pos_means(pos_name):\n",
    "    print(nltk.help.upenn_tagset(pos_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_stemmer(words, type=\"PS\", lang=\"english\", encoding=\"utf8\"):\n",
    "    '''\n",
    "    INPUT\n",
    "    words - a list of words\n",
    "    \n",
    "    OUTPUT\n",
    "    stem_words - a list of words after stemming\n",
    "    '''\n",
    "\n",
    "    supported_stemmers = [\"PS\",\"LS\",\"SS\"]\n",
    "    if type is False or type not in supported_stemmers:\n",
    "        return words\n",
    "    else:\n",
    "        stem_words = []\n",
    "        if type == \"PS\":\n",
    "            # PS means PorterStemmer\n",
    "            stemmer = PorterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word).encode(encoding))\n",
    "        if type == \"LS\":\n",
    "            # PS means LancasterStemmer\n",
    "            stemmer = LancasterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word).encode(encoding))\n",
    "        if type == \"SS\":\n",
    "            # PS means SnowballStemmer\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word).encode(encoding))\n",
    "        return stem_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    lemma_words - a list of words after lemmatization\n",
    "    '''\n",
    "    words = word_tokenizer(text)\n",
    "    lemma_words = []\n",
    "    wl = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos = find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos).encode(encoding))\n",
    "    return lemma_words\n",
    "\n",
    "\n",
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    pos = pos_tagger(word)[0][1]\n",
    "    # Adjective tags - 'JJ', 'JJR', 'JJS'    \n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v': \n",
    "        return 'v'\n",
    "    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synonums(word):\n",
    "    '''\n",
    "    INPUT\n",
    "    word - a word\n",
    "    \n",
    "    OUTPUT\n",
    "    synonyms - a set of words as the synonums of given word\n",
    "    '''\n",
    "    \n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    synonyms = set(synonyms)\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(text, n):\n",
    "    '''\n",
    "    INPUT\n",
    "    word - a string\n",
    "    \n",
    "    OUTPUT\n",
    "    a list of phrase\n",
    "    '''\n",
    "    n_grams = ngrams(word_tokenizer(text), n)\n",
    "    return [' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool() as p:\n",
    "    result = p.map(query_page, list(CrawlConfig.init_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['aloe vera gel',\n",
       "  'alcohol',\n",
       "  'alcohol wipes',\n",
       "  'airpod case',\n",
       "  'antibacterial hand soap',\n",
       "  'airpods',\n",
       "  'angel soft toilet paper',\n",
       "  'animal crossing new horizons',\n",
       "  'aa batteries',\n",
       "  'aloe vera gel 100 percent pure'],\n",
       " ['baby wipes',\n",
       "  'bidet',\n",
       "  'board games',\n",
       "  'bleach',\n",
       "  'bidet toilet seat',\n",
       "  'brita filter',\n",
       "  'brita pitcher',\n",
       "  'bounty paper towel',\n",
       "  'bluetooth earbuds',\n",
       "  'beef jerky'],\n",
       " ['clorox wipes',\n",
       "  'charmin toilet paper',\n",
       "  'clorox disinfecting wipes',\n",
       "  'clorox',\n",
       "  'cottonelle toilet paper',\n",
       "  'cat litter',\n",
       "  'charmin',\n",
       "  'canned food',\n",
       "  'car accessories',\n",
       "  'computer desk'],\n",
       " ['disinfectant wipes',\n",
       "  'disinfectant spray',\n",
       "  'dish soap',\n",
       "  'disinfecting wipes',\n",
       "  'disposable gloves',\n",
       "  'dust mask',\n",
       "  'dishwasher pods',\n",
       "  'dumbell set',\n",
       "  'dawn dish soap',\n",
       "  'disinfectant'],\n",
       " ['emergency c',\n",
       "  'elderberry syrup',\n",
       "  'emergen c',\n",
       "  'earbuds',\n",
       "  'electric guitar',\n",
       "  'ethernet cable',\n",
       "  'electric scooter',\n",
       "  'epsom salt',\n",
       "  'emergency food supply',\n",
       "  'excedrin migraine'],\n",
       " ['face mask medical',\n",
       "  'face mask',\n",
       "  'flushable wipes',\n",
       "  'face mask n95',\n",
       "  'flushable wipes for adults',\n",
       "  'fire stick',\n",
       "  'face masks disposable',\n",
       "  'fruit snacks',\n",
       "  'fish tank',\n",
       "  'futon sofa bed'],\n",
       " ['gloves disposable latex free',\n",
       "  'gloves',\n",
       "  'gatorade',\n",
       "  'gas mask',\n",
       "  'germ x hand sanitizer',\n",
       "  'germ x',\n",
       "  'gaming chair',\n",
       "  'garbage bags',\n",
       "  'germx hand sanitizer',\n",
       "  'masks for germ protection n95'],\n",
       " [],\n",
       " [],\n",
       " ['jasmine rice',\n",
       "  'jump rope',\n",
       "  'jigsaw puzzles 1000 pieces for adults',\n",
       "  'jojo rabbit',\n",
       "  'jewelry organizer',\n",
       "  'jordan shoes for men',\n",
       "  'jordan 1',\n",
       "  'jordans',\n",
       "  'jackets for men',\n",
       "  'jive pods'],\n",
       " ['kleenex tissues',\n",
       "  'kleenex',\n",
       "  'keto snacks',\n",
       "  'keyboard',\n",
       "  'kettlebell',\n",
       "  'keto',\n",
       "  'kinetic sand',\n",
       "  'kate spade',\n",
       "  'kind bars',\n",
       "  'kitchen island'],\n",
       " ['lysol spray',\n",
       "  'lysol',\n",
       "  'lysol disinfecting wipes',\n",
       "  'laundry detergent',\n",
       "  'lysol wipes',\n",
       "  'lysol spray disinfectant',\n",
       "  'led strip lights',\n",
       "  'latex gloves',\n",
       "  'louis vuitton',\n",
       "  'lego'],\n",
       " ['masks for germ protection',\n",
       "  'mask',\n",
       "  'masks for germ protection n95',\n",
       "  'medical face mask',\n",
       "  'mucinex',\n",
       "  'monitor',\n",
       "  'mouse pad',\n",
       "  'meyers hand soap',\n",
       "  'medical gloves',\n",
       "  'masks for germ protection flu'],\n",
       " ['n95 mask',\n",
       "  'n95 respirator mask',\n",
       "  'nintendo switch games',\n",
       "  'nintendo switch',\n",
       "  'n95',\n",
       "  'nitrile gloves',\n",
       "  'nyquil',\n",
       "  'nespresso capsules',\n",
       "  'newborn diapers',\n",
       "  'nyquil severe cold and flu'],\n",
       " ['office chair',\n",
       "  'oatmeal',\n",
       "  'oral thermometer',\n",
       "  'oscillococcinum',\n",
       "  'oat milk',\n",
       "  'outdoor decor',\n",
       "  'one piece swimsuits for women',\n",
       "  'oral b replacement brush heads',\n",
       "  'outdoor chairs',\n",
       "  'oregano oil'],\n",
       " ['purell hand sanitizer',\n",
       "  'paper towels',\n",
       "  'pasta',\n",
       "  'pantry items in prime pantry',\n",
       "  'purell',\n",
       "  'peanut butter',\n",
       "  'pop socket',\n",
       "  'powdered milk',\n",
       "  'pampers wipes',\n",
       "  'pulse oximeter'],\n",
       " ['quilted northern toilet paper',\n",
       "  'q tips',\n",
       "  'queen mattress',\n",
       "  'queen bed frame',\n",
       "  'queen quilt',\n",
       "  'queen sheets',\n",
       "  'qtips',\n",
       "  'quip',\n",
       "  'quilts queen size',\n",
       "  'harley quinn'],\n",
       " ['rubbing alcohol',\n",
       "  'rice',\n",
       "  'rubbing alcohol 99 percent',\n",
       "  'respirator mask',\n",
       "  'ramen noodles',\n",
       "  'ramen',\n",
       "  'rubber gloves',\n",
       "  'room decor',\n",
       "  'rise of skywalker',\n",
       "  'roku'],\n",
       " ['sanitizer',\n",
       "  'surgical mask',\n",
       "  'scott toilet paper',\n",
       "  'sanitizing wipes',\n",
       "  'soup',\n",
       "  'shower curtain',\n",
       "  'spray bottle',\n",
       "  'shampoo and conditioner',\n",
       "  'shampoo',\n",
       "  'surgical masks disposable'],\n",
       " ['toilet paper',\n",
       "  'toilet paper bulk',\n",
       "  'thermometer for adults',\n",
       "  'thermometer',\n",
       "  'tylenol',\n",
       "  'thermometer for fever',\n",
       "  'toothpaste',\n",
       "  'toilet paper angel soft',\n",
       "  'toliet papers',\n",
       "  'tylenol extra strength'],\n",
       " ['usb c cable',\n",
       "  'umbrella',\n",
       "  'usb hub',\n",
       "  'unicorn gifts for girls',\n",
       "  'uncut gems',\n",
       "  'uv light sanitizer',\n",
       "  'untamed glennon doyle',\n",
       "  'uno card game',\n",
       "  'unlocked smartphones',\n",
       "  'usb c to hdmi'],\n",
       " ['vitamin c',\n",
       "  'vitamin d',\n",
       "  'vitamin c gummies',\n",
       "  'vacuum cleaner',\n",
       "  'vicks vaporub',\n",
       "  'vape',\n",
       "  'victoria secret',\n",
       "  'vitamin e oil',\n",
       "  'viva paper towels',\n",
       "  'vape juice'],\n",
       " ['wipes',\n",
       "  'water bottle',\n",
       "  'wet ones',\n",
       "  'wireless earbuds',\n",
       "  'water filter pitcher',\n",
       "  'water bottles 24 pack',\n",
       "  'wireless mouse',\n",
       "  'womens tops',\n",
       "  'wet wipes',\n",
       "  'wet ones antibacterial wipes'],\n",
       " ['xbox one controller',\n",
       "  'xbox one',\n",
       "  'xbox gift card digital code',\n",
       "  'xbox one headset',\n",
       "  'xbox x',\n",
       "  'xiaomi mi 9t pro',\n",
       "  'xbox one games',\n",
       "  'xbox',\n",
       "  'xbox one elite controller series 2',\n",
       "  'xp pen'],\n",
       " ['yeast for bread',\n",
       "  'yeast',\n",
       "  'yeezy 350',\n",
       "  'yoga block',\n",
       "  'yeti tumbler',\n",
       "  'baby yoda',\n",
       "  'yellow dresses for women',\n",
       "  'yoni steam seat',\n",
       "  'youtube camera for vlogging',\n",
       "  'yamaha keyboard'],\n",
       " ['zinc lozenge',\n",
       "  'zinc 50mg',\n",
       "  'zapatos de hombre',\n",
       "  'zero water filter',\n",
       "  'ziploc bags',\n",
       "  'zyrtec',\n",
       "  'zapatos de mujer',\n",
       "  'zanitizer',\n",
       "  'zinus',\n",
       "  'zaful bikini'],\n",
       " []]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
