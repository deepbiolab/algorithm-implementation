{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_match_seg_cn(string, max_len=5, dic=None):\n",
    "    string = \"我们经常有意见分歧\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    max_len = 5\n",
    "    while i < len(string):\n",
    "        maxWord = \"\"\n",
    "        for j in range(i, i+max_len):\n",
    "            tempWord = string[i:j+1]\n",
    "            if tempWord in dic and len(tempWord) > len(maxWord):\n",
    "                maxWord = tempWord\n",
    "        i = i+len(maxWord)\n",
    "        tokens.append(maxWord)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_len 需要对词典的长度做分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class ForwardMaxMatch:\n",
    "    def __init__(self, dic='./words.dic', max_len=5):\n",
    "        self.max_len = max_len\n",
    "        self.dic_file = dic\n",
    "        self.dic = self.load_dic()\n",
    "        self.tokens = []\n",
    "        \n",
    "    def load_dic(self):\n",
    "        with open(self.dic_file, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            dic = [i.strip() for i in data]\n",
    "            return dic   \n",
    "    \n",
    "    def segment(self, word_obj):\n",
    "        if isinstance(word_obj, str):\n",
    "            character_map = {\".\": \" . \",\n",
    "                             \",\": ' , '}\n",
    "            for origin, new in character_map.items():\n",
    "                word_obj = word_obj.replace(origin, new)\n",
    "            self.words_list =word_obj.lower().split()\n",
    "        elif isinstance(word_obj, list):\n",
    "            self.words_list = word_obj\n",
    "        else:\n",
    "            print(\"Not support object for segmentation!\")\n",
    "        i = 0\n",
    "        tokens = []\n",
    "        while i < len(self.words_list):\n",
    "            maxWords = []\n",
    "            reverse = self.max_len + i\n",
    "            while reverse > i:   \n",
    "                grams = self.words_list[i: reverse]\n",
    "                reverse -= 1\n",
    "                tempWords = ' '.join(grams)\n",
    "                if tempWords in self.dic:\n",
    "                    maxWords = grams\n",
    "                    break\n",
    "            if maxWords:\n",
    "                i += len(maxWords)\n",
    "                tokens.append(' '.join(maxWords))\n",
    "            else:\n",
    "                tokens.append(' '.join(self.words_list[i: i+1]))\n",
    "                i += 1\n",
    "        self.tokens = tokens\n",
    "        return tokens\n",
    "    \n",
    "    def __call__(self, word_obj):\n",
    "        return self.segment(word_obj)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class BackwardMaxMatch:\n",
    "    def __init__(self, dic='./words.dic', max_len=5):\n",
    "        self.max_len = max_len\n",
    "        self.dic_file = dic\n",
    "        self.dic = self.load_dic()\n",
    "        self.tokens = []\n",
    "        \n",
    "    def load_dic(self):\n",
    "        with open(self.dic_file, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            dic = [i.strip() for i in data]\n",
    "            return dic   \n",
    "    \n",
    "    def segment(self, word_obj):\n",
    "        if isinstance(word_obj, str):\n",
    "            character_map = {\".\": \" . \",\n",
    "                             \",\": ' , '}\n",
    "            for origin, new in character_map.items():\n",
    "                word_obj = word_obj.replace(origin, new)\n",
    "            self.words_list =word_obj.lower().split()\n",
    "        elif isinstance(word_obj, list):\n",
    "            self.words_list = word_obj\n",
    "        else:\n",
    "            print(\"Not support object for segmentation!\")\n",
    "        i = -1\n",
    "        tokens = []\n",
    "        while i > -len(self.words_list):\n",
    "            maxWords = []\n",
    "            reverse = -self.max_len + i\n",
    "            while reverse < i:   \n",
    "                grams = self.words_list[reverse: i]\n",
    "                reverse += 1\n",
    "                tempWords = ' '.join(grams)\n",
    "                if tempWords in self.dic:\n",
    "                    maxWords = grams\n",
    "                    break\n",
    "            if maxWords:\n",
    "                i -= len(maxWords)\n",
    "                tokens.append(' '.join(maxWords))\n",
    "            else:\n",
    "                tokens.append(' '.join(self.words_list[i-1: i]))\n",
    "                i -= 1\n",
    "        self.tokens = list(reversed(tokens))\n",
    "        if self.words_list[-1] == '.': self.tokens.append('.')\n",
    "        return self.tokens\n",
    "    \n",
    "    def __call__(self, word_obj):\n",
    "        return self.segment(word_obj)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidirectMaxMatch(string):\n",
    "    back_tokenizer = BackwardMaxMatch()\n",
    "    back_tokens = back_tokenizer.segment(string)\n",
    "    forward_tokenizer = ForwardMaxMatch()\n",
    "    forward_tokens = forward_tokenizer.segment(string)\n",
    "    \n",
    "    if len(back_tokens) == len(forward_tokens):\n",
    "        if back_tokens == forward_tokens:\n",
    "            return back_tokens\n",
    "        else:\n",
    "            back_single_w_cnt = sum([0 if len(bt.split(' ')) > 1 else 1 for bt in back_tokens])\n",
    "            forward_single_w_cnt = sum([0 if len(bt.split(' ')) > 1 else 1 for bt in forward_tokens])\n",
    "            if back_single_w_cnt < forward_single_w_cnt:\n",
    "                return back_tokens\n",
    "            else:\n",
    "                return forward_tokens\n",
    "    else:\n",
    "        if len(back_tokens) < len(forward_tokens):\n",
    "            return back_tokens\n",
    "        else:\n",
    "            return forward_tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"Learn more about Machine Learning, an application of AI that provides \n",
    "systems the ability to automatically learn and improve from experience.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'more',\n",
       " 'about',\n",
       " 'machine learning',\n",
       " ',',\n",
       " 'an application',\n",
       " 'of',\n",
       " 'ai',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'systems',\n",
       " 'the ability',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'from',\n",
       " 'experience',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_tokenizer = BackwardMaxMatch()\n",
    "back_tokens = back_tokenizer(string)\n",
    "back_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'more',\n",
       " 'about',\n",
       " 'machine learning',\n",
       " ',',\n",
       " 'an application',\n",
       " 'of',\n",
       " 'ai',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'systems',\n",
       " 'the ability',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'from',\n",
       " 'experience',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ForwardMaxMatch()\n",
    "tokenizer(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn',\n",
       " 'more',\n",
       " 'about',\n",
       " 'machine learning',\n",
       " ',',\n",
       " 'an application',\n",
       " 'of',\n",
       " 'ai',\n",
       " 'that',\n",
       " 'provides',\n",
       " 'systems',\n",
       " 'the ability',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'improve',\n",
       " 'from',\n",
       " 'experience',\n",
       " '.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BidirectMaxMatch(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
